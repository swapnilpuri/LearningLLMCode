{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c478dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba0bca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # Load environment variables from .env file\n",
    "openai_client = OpenAI()  # Initialize OpenAI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b33514d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(\"knowledgebase/Resume-Swapnil Puri_Full_SS.pdf\")\n",
    "resume = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        resume += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08de925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68a8b8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Extracted Resume Text:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Resume-Swapnil Puri \n",
       "swapnilpuri@hotmail.com \n",
       "Ph: 856-313-2878 \n",
       " \n",
       "Educational Qualification: B.Sc. from College of Science, MLS University, \n",
       "Udaipur, India (Passed out 1997) \n",
       " \n",
       "Experience Summary  \n",
       "Over 17 Years with the following companies:  \n",
       "1. State Street Corp., USA from Feb 2015 – till date \n",
       "2. WIPRO Technologies, USA from July, 2013 – Jan, 2015   \n",
       "3. Prolifics, US from Feb 2012 – June, 2013 \n",
       "4. Infosys Technologies, USA from Nov 2010 – Feb 2012  \n",
       "5. WIPRO Technologies, Chennai from April 2..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"### Extracted Resume Text:\"))\n",
    "display(Markdown(resume[:500] + \"...\"))  # Display first 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "219ae461",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"knowledgebase/summary.txt\", \"r\") as file:\n",
    "    summary = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b55e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Swapnil Puri\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "763a829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"\n",
    "You are an expert career counselor. You have the following resume of a person named {name}\n",
    "{resume}. You are answering questions based on this resume only. Also, you have the following summary of this person's career objectives and skills:\n",
    "{summary}. Use this information to answer the questions accurately. The Agent has been instructed to be professional and engaging\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9767c737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_resume(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}]+ history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1393b138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(chat_with_resume, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "124f5806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42a14b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_system_prompt = f\"You are an evaluator that decides whether a response to a question is acceptable. \\\n",
    "You are provided with a conversation between a User and an Agent. Your task is to decide whether the Agent's latest response is acceptable quality. \\\n",
    "The Agent is playing the role of an expert career counselor. You have the following resume of a person named {name} \\\n",
    "{resume}. The Agent has been instructed to be professional and engaging. \\\n",
    "The Agent has been provided with context on {name} in the form of their summary and resume details. Here's the information:\"\n",
    "\n",
    "evaluator_system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## Resume:\\n{resume}\\n\\n\"\n",
    "evaluator_system_prompt += f\"With this context, please evaluate the latest response, replying with whether the response is acceptable and your feedback.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3262c235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_user_prompt(reply, message, history):\n",
    "    user_prompt = f\"Here's the conversation between the User and the Agent: \\n\\n{history}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest message from the User: \\n\\n{message}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest response from the Agent: \\n\\n{reply}\\n\\n\"\n",
    "    user_prompt += \"Please evaluate the response, replying with whether it is acceptable and your feedback.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d443e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gemini = OpenAI(\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\"), \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90d0e1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(reply, message, history) -> Evaluation:\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}] + [{\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}]\n",
    "    response = gemini.beta.chat.completions.parse(model=\"gemini-2.0-flash\", messages=messages, response_format=Evaluation)\n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02faf283",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": system_prompt}] + [{\"role\": \"user\", \"content\": \"do you hold any certifications?\"}]\n",
    "response = openai_client.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "reply = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14050419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Reply:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Yes, I hold several certifications, including:\n",
       "\n",
       "1. Sun Certified Java Programmer\n",
       "2. Sun Certified Web Component Developer\n",
       "3. Sun Certified Business Component Developer\n",
       "4. Sun Certified Enterprise Architect\n",
       "5. IBM Certified Solution Developer - Websphere Message Broker V6.0\n",
       "6. IBM Certified Solution Developer - Websphere Message Broker V7.0\n",
       "7. IBM Certified Solution Designer - WebSphere MQ V6.0\n",
       "8. IBM WebSphere DataPower SOA Appliances Firmware V3.8.1, Solution Implementation\n",
       "9. IBM WebSphere Enterprise Service Bus V7.0, Integration Development\n",
       "10. TOGAF 9.1 Certification\n",
       "11. SOA Certified Professional from SOA School\n",
       "\n",
       "These certifications reflect my commitment to continuous learning and expertise in the technology field."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"### Reply:\"))\n",
    "display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb315dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Evaluation(is_acceptable=True, feedback='The agent has provided a perfect response.')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(reply, \"do you hold any certifications?\", messages[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aad1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerun(reply, message, history, feedback):\n",
    "    updated_system_prompt = system_prompt + \"\\n\\n## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply\\n\"\n",
    "    updated_system_prompt += f\"## Your attempted answer:\\n{reply}\\n\\n\"\n",
    "    updated_system_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
    "    messages = [{\"role\": \"system\", \"content\": updated_system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai_client.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5949d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    if \"patent\" in message:\n",
    "        system = system_prompt + \"\\n\\nEverything in your reply needs to be in pig latin - \\\n",
    "              it is mandatory that you respond only and entirely in pig latin\"\n",
    "    else:\n",
    "        system = system_prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai_client.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    reply =response.choices[0].message.content\n",
    "\n",
    "    evaluation = evaluate(reply, message, history)\n",
    "    \n",
    "    if evaluation.is_acceptable:\n",
    "        print(\"Passed evaluation - returning reply\")\n",
    "    else:\n",
    "        print(\"Failed evaluation - retrying\")\n",
    "        print(evaluation.feedback)\n",
    "        reply = rerun(reply, message, history, evaluation.feedback)       \n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd8f6bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed evaluation - returning reply\n",
      "Passed evaluation - returning reply\n",
      "Failed evaluation - retrying\n",
      "This answer is unacceptable. The Agent has answered in pig latin, which is unprofessional. The agent should say that based on the resume it is not possible to answer the question.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Swapnil\\Code\\GenAI\\eddoner\\LearningLLMCode\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 759, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Swapnil\\Code\\GenAI\\eddoner\\LearningLLMCode\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Swapnil\\Code\\GenAI\\eddoner\\LearningLLMCode\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2116, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Swapnil\\Code\\GenAI\\eddoner\\LearningLLMCode\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1621, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Swapnil\\Code\\GenAI\\eddoner\\LearningLLMCode\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 882, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Swapnil\\Code\\GenAI\\eddoner\\LearningLLMCode\\.venv\\Lib\\site-packages\\gradio\\chat_interface.py\", line 553, in __wrapper\n",
      "    return await submit_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Swapnil\\Code\\GenAI\\eddoner\\LearningLLMCode\\.venv\\Lib\\site-packages\\gradio\\chat_interface.py\", line 943, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Swapnil\\Code\\GenAI\\eddoner\\LearningLLMCode\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Swapnil\\Code\\GenAI\\eddoner\\LearningLLMCode\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2485, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Swapnil\\Code\\GenAI\\eddoner\\LearningLLMCode\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 976, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_27328\\987233366.py\", line 18, in chat\n",
      "    reply = rerun(reply, message, history, evaluation.feedback)\n",
      "     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_27328\\1832164519.py\", line 6, in rerun\n",
      "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
      "               ^^^^^^\n",
      "NameError: name 'openai' is not defined. Did you mean: 'OpenAI'?\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LearningLLMCode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
